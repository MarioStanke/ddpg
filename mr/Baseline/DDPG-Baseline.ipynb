{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG Implementation using tf agents library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from tf_agents.agents.ddpg import actor_network\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from gym.envs import box2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "env_name = 'BipedalWalker-v2'\n",
    "num_iterations = 2500000\n",
    "use_tf_functions = True\n",
    "\n",
    "# Replay Buffer Parameters & Noise Function Parameters\n",
    "initial_collect_steps = 1000 \n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_capacity = 100000\n",
    "ou_stddev = 0.2 \n",
    "ou_damping = 0.15 \n",
    "\n",
    "# Target Update Parameters\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "\n",
    "# Train Step Parameters\n",
    "train_steps_per_iteration = 1 \n",
    "batch_size = 64\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "td_errors_loss_fn = tf.compat.v1.losses.mean_squared_error \n",
    "gamma = 0.99 \n",
    "reward_scale_factor = 1.0 \n",
    "\n",
    "# Evaluation and Summary Parameters\n",
    "num_eval_episodes = 100\n",
    "eval_interval = 10000\n",
    "log_interval = 1000\n",
    "summary_interval = 1000\n",
    "summaries_flush_secs = 10\n",
    "run_id = 20420    # ID to differentiate between runs\n",
    "root_dir = '~/Bachelorarbeit/Baseline'    # Has to be an existing directory\n",
    "\n",
    "# For training on Brain\n",
    "use_brain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_brain:\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser(description = 'DDPG Arguments')\n",
    "    parser.add_argument('--run_id', type = int, help = \"identifying substring for folder names (default: date)\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.run_id is not None:\n",
    "        run_id = args.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py:201: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.while_loop(c, b, vars, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1783: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1783: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=120.05078>, extra=DdpgInfo(actor_loss=<tf.Tensor: shape=(), dtype=float32, numpy=82.62599>, critic_loss=<tf.Tensor: shape=(), dtype=float32, numpy=37.42479>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DDPG_Bipedal(root_dir):\n",
    "    \n",
    "    # Setting up directories for results\n",
    "    root_dir = os.path.expanduser(root_dir)\n",
    "    train_dir = os.path.join(root_dir, 'train' + '/' + str(run_id))\n",
    "    eval_dir = os.path.join(root_dir, 'eval' + '/' + str(run_id))\n",
    "    \n",
    "    # Set up Summary writer for training and evaluation\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        train_dir, flush_millis = summaries_flush_secs * 1000\n",
    "    )\n",
    "    train_summary_writer.set_as_default()\n",
    "    \n",
    "    eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        eval_dir, flush_millis = summaries_flush_secs * 1000\n",
    "    )\n",
    "    eval_metrics = [\n",
    "        # Metric to record average return\n",
    "        tf_metrics.AverageReturnMetric(buffer_size = num_eval_episodes),\n",
    "        # Metric to record average episode length\n",
    "        tf_metrics.AverageEpisodeLengthMetric(buffer_size = num_eval_episodes)\n",
    "    ]\n",
    "    \n",
    "    #Create global step\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    \n",
    "    with tf.compat.v2.summary.record_if(\n",
    "        lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "        # Load Environment with different wrappers\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "        eval_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "        eval_py_env = suite_gym.load(env_name)\n",
    "    \n",
    "    \n",
    "        # Define Actor Network\n",
    "        actorNN = actor_network.ActorNetwork(\n",
    "                  tf_env.time_step_spec().observation,\n",
    "                  tf_env.action_spec(),\n",
    "                  fc_layer_params=(400, 300),\n",
    "        )\n",
    "    \n",
    "        # Define Critic Network\n",
    "        NN_input_specs = (tf_env.time_step_spec().observation,\n",
    "                          tf_env.action_spec()\n",
    "        )\n",
    "    \n",
    "        criticNN = critic_network.CriticNetwork(\n",
    "                   NN_input_specs,\n",
    "                   observation_fc_layer_params = (400,),\n",
    "                   action_fc_layer_params = None,\n",
    "                   joint_fc_layer_params = (300,),\n",
    "        )\n",
    "        \n",
    "        # Define & initialize DDPG Agent\n",
    "        agent = ddpg_agent.DdpgAgent(\n",
    "                tf_env.time_step_spec(),\n",
    "                tf_env.action_spec(),\n",
    "                actor_network = actorNN,\n",
    "                critic_network = criticNN,\n",
    "                actor_optimizer = tf.compat.v1.train.AdamOptimizer(\n",
    "                                  learning_rate = actor_learning_rate),\n",
    "                critic_optimizer = tf.compat.v1.train.AdamOptimizer(\n",
    "                                   learning_rate = critic_learning_rate),\n",
    "                ou_stddev = ou_stddev,\n",
    "                ou_damping = ou_damping,\n",
    "                target_update_tau = target_update_tau,\n",
    "                target_update_period = target_update_period,\n",
    "                dqda_clipping = None,\n",
    "                td_errors_loss_fn = tf.compat.v1.losses.mean_squared_error,\n",
    "                gamma = gamma,\n",
    "                reward_scale_factor = 1.0,\n",
    "                gradient_clipping = None,\n",
    "                debug_summaries = False,\n",
    "                summarize_grads_and_vars = False,\n",
    "                train_step_counter = global_step\n",
    "        )\n",
    "        agent.initialize()\n",
    "        \n",
    "        # Determine which train metrics to display with summary writer\n",
    "        train_metrics = [\n",
    "            tf_metrics.NumberOfEpisodes(),\n",
    "            tf_metrics.EnvironmentSteps(),\n",
    "            tf_metrics.AverageReturnMetric(),\n",
    "            tf_metrics.AverageEpisodeLengthMetric(),\n",
    "        ]\n",
    "        \n",
    "        # Set policies for evaluation and initial collection\n",
    "        eval_policy = agent.policy_state    # Actor policy\n",
    "        collect_policy = agent.collect_policy    # Actor policy with OUNoise\n",
    "        \n",
    "        # Set up replay buffer\n",
    "        replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "                        agent.collect_data_spec,\n",
    "                        batch_size = tf_env.batch_size,\n",
    "                        max_length = replay_buffer_capacity\n",
    "        )\n",
    "        \n",
    "        # Define driver for initial replay buffer filling\n",
    "        initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "                                     tf_env,\n",
    "                                     collect_policy,    # Initializes with random Parameters as beta\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_steps = initial_collect_steps\n",
    "        )\n",
    "\n",
    "        # Define collect driver for collect steps per iteration\n",
    "        collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "                             tf_env,\n",
    "                             collect_policy,\n",
    "                             observers = [replay_buffer.add_batch] + train_metrics,\n",
    "                             num_steps = collect_steps_per_iteration\n",
    "        )\n",
    "        \n",
    "        if use_tf_functions:\n",
    "            initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "            collect_driver.run = common.function(collect_driver.run)\n",
    "            agent.train = common.function(agent.train)\n",
    "            \n",
    "        # Make 1000 random steps in tf_env and save in Replay Buffer\n",
    "        logging.info(\n",
    "            'Initializing replay buffer by collecting experience for 1000 steps with '\n",
    "            'a random policy.', initial_collect_steps)\n",
    "        initial_collect_driver.run()\n",
    "        \n",
    "        # Computes Evaluation Metrics\n",
    "        results = metric_utils.eager_compute(\n",
    "                  eval_metrics,\n",
    "                  eval_tf_env,\n",
    "                  eval_policy,\n",
    "                  num_episodes = num_eval_episodes,\n",
    "                  train_step = global_step,\n",
    "                  summary_writer = eval_summary_writer,\n",
    "                  summary_prefix = 'Metrics',\n",
    "        )\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "        \n",
    "        time_step = None\n",
    "        policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0 \n",
    "\n",
    "        # Dataset outputs steps in batches of 64\n",
    "        dataset = replay_buffer.as_dataset(\n",
    "                  num_parallel_calls = 3,\n",
    "                  sample_batch_size = 64,\n",
    "                  num_steps = 2).prefetch(3)\n",
    "        iterator = iter(dataset)\n",
    "\n",
    "        def train_step():\n",
    "            experience, _ = next(iterator) #Get experience from dataset (replay buffer)\n",
    "            return agent.train(experience) #Train agent on that experience\n",
    "        \n",
    "        if use_tf_functions:\n",
    "            train_step = common.function(train_step)\n",
    "            \n",
    "        \n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time() # Get start time\n",
    "            # Collect data for replay buffer\n",
    "            time_step, policy_state = collect_driver.run(\n",
    "                                      time_step = time_step,\n",
    "                                      policy_state = policy_state,\n",
    "            )\n",
    "            # Train on experience \n",
    "            for _ in range(train_steps_per_iteration):\n",
    "                train_loss = train_step()\n",
    "            time_acc += time.time() - start_time\n",
    "\n",
    "            if global_step.numpy() % log_interval == 0:\n",
    "                logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                             train_loss.loss\n",
    "                )\n",
    "                steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "                logging.info('%.3f steps/sec', steps_per_sec)\n",
    "                tf.compat.v2.summary.scalar(\n",
    "                    name = 'global_steps_per_sec', data = steps_per_sec, \n",
    "                    step = global_step\n",
    "                )\n",
    "                timed_at_step = global_step.numpy()\n",
    "                time_acc = 0\n",
    "\n",
    "            for train_metric in train_metrics:\n",
    "                train_metric.tf_summaries(train_step = global_step, \n",
    "                                          step_metrics = train_metrics[:2])\n",
    "                \n",
    "            if global_step.numpy() % eval_interval == 0:\n",
    "                results = metric_utils.eager_compute(\n",
    "                          eval_metrics,\n",
    "                          eval_tf_env,\n",
    "                          eval_policy,\n",
    "                          num_episodes = num_eval_episodes,\n",
    "                          train_step = global_step,\n",
    "                          summary_writer = eval_summary_writer,\n",
    "                          summary_prefix = 'Metrics',\n",
    "                )\n",
    "                if eval_metrics_callback is not None:\n",
    "                    eval_metrics_callback(results, global_step.numpy())\n",
    "                metric_utils.log_metrics(eval_metrics)\n",
    "                \n",
    "    return train_loss\n",
    "\n",
    "DDPG_Bipedal(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
