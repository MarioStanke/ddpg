{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports here\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "\n",
    "import imageio\n",
    "import io\n",
    "import gin\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from tf_agents.agents.ddpg import actor_network\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from gym.envs import box2d\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defaults\n",
    "env_name = 'BipedalWalker-v2'\n",
    "num_iterations = 3000000\n",
    "# Params for collect\n",
    "initial_collect_steps = 1000 #1000\n",
    "collect_steps_per_iteration = 1\n",
    "num_parallel_environments = 1\n",
    "replay_buffer_capacity = 100000\n",
    "ou_stddev = 0.2 #0.2\n",
    "ou_damping = 0.15 #0.15\n",
    "# Params for target update\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "# Params for train\n",
    "train_steps_per_iteration = 1 #1\n",
    "batch_size = 64\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "dqda_clipping = None\n",
    "td_errors_loss_fn = tf.compat.v1.losses.mean_squared_error #tf.compat.v1.losses.huber_loss potential problem? MSE?\n",
    "gamma = 0.99 #0.995\n",
    "reward_scale_factor = 1.0\n",
    "gradient_clipping = None\n",
    "use_tf_functions = True\n",
    "# Params for eval\n",
    "num_eval_episodes = 10\n",
    "eval_interval = 10000\n",
    "eval_metrics_callback = None\n",
    "log_interval = 1000\n",
    "summary_interval = 1000\n",
    "summaries_flush_secs = 10\n",
    "run_id = 210120\n",
    "root_dir = '~/'\n",
    "\n",
    "#For Brain\n",
    "use_brain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_brain:\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser(description = 'DDPG Arguments')\n",
    "    parser.add_argument('--run_id', type = int, help = \"identifying substring for folder names (default: date)\")\n",
    "    parser.add_argument('--root_dir', help = \"directory for output \")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.run_id is not None:\n",
    "        run_id = args.run_id\n",
    "    if root_dir is not None\n",
    "        root_dir = args.root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDPG_Bipedal(root_dir):\n",
    "    \n",
    "    #Setting up directories for log and evaluation\n",
    "    root_dir = os.path.expanduser(root_dir)\n",
    "    train_dir = os.path.join(root_dir, 'train' + str(run_id))\n",
    "    eval_dir = os.path.join(root_dir, 'eval' + str(run_id))\n",
    "    video_dir = os.path.join(root_dir, 'vid' + str(run_id))\n",
    "    \n",
    "    #Set up train summary writer and eval summary writer\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        train_dir, flush_millis = summaries_flush_secs * 1000\n",
    "    )\n",
    "    train_summary_writer.set_as_default()\n",
    "    \n",
    "    eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        eval_dir, flush_millis = summaries_flush_secs * 1000\n",
    "    )\n",
    "    eval_metrics = [\n",
    "        tf_metrics.AverageReturnMetric(buffer_size = num_eval_episodes), #metric to compute av return\n",
    "        tf_metrics.AverageEpisodeLengthMetric(buffer_size = num_eval_episodes) #metric to compute av ep length\n",
    "    ]\n",
    "    \n",
    "    #Create global step\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    \n",
    "    with tf.compat.v2.summary.record_if(\n",
    "        lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "        eval_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "        eval_py_env = suite_gym.load(env_name)\n",
    "    \n",
    "    \n",
    "        #Define Actor Network\n",
    "        actorNN = actor_network.ActorNetwork(\n",
    "                  tf_env.time_step_spec().observation,\n",
    "                  tf_env.action_spec(),\n",
    "                  fc_layer_params=(400, 300),\n",
    "        )\n",
    "    \n",
    "        #Define Critic Network\n",
    "        NN_input_specs = (tf_env.time_step_spec().observation,\n",
    "                          tf_env.action_spec()\n",
    "        )\n",
    "    \n",
    "        criticNN = critic_network.CriticNetwork(\n",
    "                   NN_input_specs,\n",
    "                   observation_fc_layer_params = (400,),\n",
    "                   action_fc_layer_params = None,\n",
    "                   joint_fc_layer_params = (300,),\n",
    "        )\n",
    "        \n",
    "        #Define & initialize DDPG Agent\n",
    "        agent = ddpg_agent.DdpgAgent(\n",
    "                tf_env.time_step_spec(),\n",
    "                tf_env.action_spec(),\n",
    "                actor_network = actorNN,\n",
    "                critic_network = criticNN,\n",
    "                actor_optimizer = tf.compat.v1.train.AdamOptimizer(\n",
    "                                  learning_rate = actor_learning_rate),\n",
    "                critic_optimizer = tf.compat.v1.train.AdamOptimizer(\n",
    "                                   learning_rate = critic_learning_rate),\n",
    "                ou_stddev = ou_stddev,\n",
    "                ou_damping = ou_damping,\n",
    "                target_update_tau = target_update_tau,\n",
    "                target_update_period = target_update_period,\n",
    "                dqda_clipping = None,\n",
    "                td_errors_loss_fn = tf.compat.v1.losses.mean_squared_error,\n",
    "                gamma = gamma,\n",
    "                reward_scale_factor = 1.0,\n",
    "                gradient_clipping = None,\n",
    "                debug_summaries = False,\n",
    "                summarize_grads_and_vars = False,\n",
    "                train_step_counter = global_step\n",
    "        )\n",
    "        agent.initialize()\n",
    "        \n",
    "        #Determine which train metrics to display with summary writer\n",
    "        train_metrics = [\n",
    "            tf_metrics.NumberOfEpisodes(),\n",
    "            tf_metrics.EnvironmentSteps(),\n",
    "            tf_metrics.AverageReturnMetric(),\n",
    "            tf_metrics.AverageEpisodeLengthMetric(),\n",
    "        ]\n",
    "        \n",
    "        #Set policies for evaluation and initial collection\n",
    "        eval_policy = agent.policy #actor policy\n",
    "        collect_policy = agent.collect_policy #actor policy with OUNoise\n",
    "        \n",
    "        #Set up replay buffer\n",
    "        replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "                        agent.collect_data_spec,\n",
    "                        batch_size = tf_env.batch_size,\n",
    "                        max_length = replay_buffer_capacity\n",
    "        )\n",
    "        \n",
    "        #Define driver for initial replay buffer filling\n",
    "        initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "                                     tf_env,\n",
    "                                     collect_policy,\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_steps = initial_collect_steps\n",
    "        )\n",
    "\n",
    "        #Define collect driver for collect steps per iteration\n",
    "        collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "                             tf_env,\n",
    "                             collect_policy,\n",
    "                             observers = [replay_buffer.add_batch] + train_metrics,\n",
    "                             num_steps = collect_steps_per_iteration\n",
    "        )\n",
    "        \n",
    "        if use_tf_functions:\n",
    "            initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "            collect_driver.run = common.function(collect_driver.run)\n",
    "            agent.train = common.function(agent.train)\n",
    "            \n",
    "        # Collect initial replay data\n",
    "        logging.info(\n",
    "            'Initializing replay buffer by collecting experience for %d steps with '\n",
    "            'a random policy.', initial_collect_steps)\n",
    "        initial_collect_driver.run()\n",
    "        \n",
    "        #Computes Evaluation Metrics\n",
    "        results = metric_utils.eager_compute(\n",
    "                  eval_metrics,\n",
    "                  eval_tf_env,\n",
    "                  eval_policy,\n",
    "                  num_episodes = num_eval_episodes,\n",
    "                  train_step = global_step,\n",
    "                  summary_writer = eval_summary_writer,\n",
    "                  summary_prefix = 'Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "            eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "        \n",
    "        time_step = None\n",
    "        policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0 \n",
    "\n",
    "        # Dataset generates trajectories with shape [Bx2x...]\n",
    "        dataset = replay_buffer.as_dataset(\n",
    "                  num_parallel_calls = 3,\n",
    "                  sample_batch_size = 64,\n",
    "                  num_steps = 2).prefetch(3)\n",
    "        iterator = iter(dataset)\n",
    "\n",
    "        def train_step():\n",
    "            experience, _ = next(iterator) #Get experience from dataset\n",
    "            return agent.train(experience) #Train agent on that experience\n",
    "        \n",
    "        if use_tf_functions:\n",
    "            train_step = common.function(train_step)\n",
    "            \n",
    "        \n",
    "        #Where the magic happens\n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time() #Get start time\n",
    "            #Collect some data for replay buffer (also observed by train metrics)\n",
    "            time_step, policy_state = collect_driver.run(\n",
    "                                      time_step = time_step,\n",
    "                                      policy_state = policy_state,\n",
    "            )\n",
    "            #Train on collected experience n times\n",
    "            for _ in range(train_steps_per_iteration):\n",
    "                train_loss = train_step()\n",
    "            time_acc += time.time() - start_time\n",
    "\n",
    "            if global_step.numpy() % log_interval == 0:\n",
    "                logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                             train_loss.loss\n",
    "                )\n",
    "                steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "                logging.info('%.3f steps/sec', steps_per_sec)\n",
    "                tf.compat.v2.summary.scalar(\n",
    "                    name = 'global_steps_per_sec', data = steps_per_sec, \n",
    "                    step = global_step\n",
    "                )\n",
    "                timed_at_step = global_step.numpy()\n",
    "                time_acc = 0\n",
    "\n",
    "            for train_metric in train_metrics:\n",
    "                train_metric.tf_summaries(train_step = global_step, \n",
    "                                          step_metrics = train_metrics[:2])\n",
    "                \n",
    "            if global_step.numpy() % eval_interval == 0:\n",
    "                results = metric_utils.eager_compute(\n",
    "                          eval_metrics,\n",
    "                          eval_tf_env,\n",
    "                          eval_policy,\n",
    "                          num_episodes = num_eval_episodes,\n",
    "                          train_step = global_step,\n",
    "                          summary_writer = eval_summary_writer,\n",
    "                          summary_prefix = 'Metrics',\n",
    "                )\n",
    "                if eval_metrics_callback is not None:\n",
    "                    eval_metrics_callback(results, global_step.numpy())\n",
    "                metric_utils.log_metrics(eval_metrics)\n",
    "                if results['AverageReturn'].numpy() >= 270.0:\n",
    "                    num_episodes = 5\n",
    "                    frames = []\n",
    "                    for _ in range(num_episodes):\n",
    "                        time_step = eval_tf_env.reset()\n",
    "                        frames.append(eval_py_env.render())\n",
    "                        while not time_step.is_last():\n",
    "                            action_step = eval_policy.action(time_step)\n",
    "                            time_step = eval_tf_env.step(action_step.action)\n",
    "                            fnally = action_step[0].numpy()\n",
    "                            next_state, reward, done, _ = eval_py_env.step(fnally[0])\n",
    "                            frames.append(eval_py_env.render())\n",
    "                        eval_py_env.close()\n",
    "                        eval_tf_env.close()\n",
    "                    gif_file = root_dir + \"/\" + run_id + \"-\" + str(global_step.numpy()) + '.gif'\n",
    "                    imageio.mimsave(gif_file, frames, format = 'gif', fps = 60)\n",
    "            \n",
    "    return train_loss\n",
    "\n",
    "\n",
    "DDPG_Bipedal(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python <myenv>",
   "language": "python",
   "name": "tf2rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
