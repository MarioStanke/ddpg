{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDPG Implementation using tf agents library\n",
    "#Uses ffmpeg for video creation, Video creation might still be faulty\n",
    "#Faulty Implementation of Idee 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from tf_agents.agents.ddpg import actor_network\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent #from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from gym.envs import box2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defaults\n",
    "env_name = 'BipedalWalker-v2'\n",
    "num_iterations = 2500000\n",
    "# Params for collect\n",
    "initial_collect_steps = 1000 #1000\n",
    "collect_steps_per_iteration = 5 #1\n",
    "num_parallel_environments = 1\n",
    "replay_buffer_capacity = 100000\n",
    "ou_stddev = 0.2 #0.2\n",
    "ou_damping = 0.15 #0.15\n",
    "# Params for target update\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "# Params for train\n",
    "train_steps_per_iteration = 1 #1\n",
    "batch_size = 64\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "dqda_clipping = None\n",
    "td_errors_loss_fn = tf.compat.v1.losses.mean_squared_error #tf.compat.v1.losses.huber_loss potential problem? MSE?\n",
    "gamma = 0.995 #0.995\n",
    "reward_scale_factor = 1.0\n",
    "gradient_clipping = None\n",
    "use_tf_functions = True\n",
    "# Params for eval\n",
    "num_eval_episodes = 100\n",
    "eval_interval = 10000\n",
    "eval_metrics_callback = None\n",
    "log_interval = 1000\n",
    "summary_interval = 1000\n",
    "summaries_flush_secs = 10\n",
    "run_id = 18202\n",
    "root_dir = '~/' #'~/'\n",
    "\n",
    "#For Brain\n",
    "use_brain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_brain:\n",
    "    global args\n",
    "    parser = argparse.ArgumentParser(description = 'DDPG Arguments')\n",
    "    parser.add_argument('--run_id', type = int, help = \"identifying substring for folder names (default: date)\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.run_id is not None:\n",
    "        run_id = args.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\m-ohm\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\m-ohm\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3f23f98dc69d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m \u001b[0mDDPG_Bipedal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-3f23f98dc69d>\u001b[0m in \u001b[0;36mDDPG_Bipedal\u001b[1;34m(root_dir)\u001b[0m\n\u001b[0;32m    203\u001b[0m                           \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                           \u001b[0msummary_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_summary_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                           \u001b[0msummary_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Metrics'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m                 )\n\u001b[0;32m    207\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meval_metrics_callback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tf_agents\\eval\\metric_utils.py\u001b[0m in \u001b[0;36meager_compute\u001b[1;34m(metrics, environment, policy, num_episodes, train_step, summary_writer, summary_prefix, use_function)\u001b[0m\n\u001b[0;32m    158\u001b[0m       num_episodes=num_episodes)\n\u001b[0;32m    159\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0muse_function\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    524\u001b[0m               *args, **kwds)\n\u001b[0;32m    525\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def DDPG_Bipedal(root_dir):\n",
    "\n",
    "    #Experience and counter for Idee 2\n",
    "    global experience\n",
    "    global counter \n",
    "    counter = 0\n",
    "    \n",
    "    #Setting up directories for log and evaluation\n",
    "    root_dir = os.path.expanduser(root_dir)\n",
    "    train_dir = os.path.join(root_dir, 'train' + '/' + str(run_id))\n",
    "    eval_dir = os.path.join(root_dir, 'eval' + '/' + str(run_id))\n",
    "    video_dir = os.path.join(root_dir, 'vid' + '/' + str(run_id))\n",
    "    \n",
    "    #Set up train summary writer and eval summary writer\n",
    "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        train_dir, flush_millis = summaries_flush_secs * 1000\n",
    "    )\n",
    "    train_summary_writer.set_as_default()\n",
    "    \n",
    "    eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "        eval_dir, flush_millis = summaries_flush_secs * 1000\n",
    "    )\n",
    "    eval_metrics = [\n",
    "        tf_metrics.AverageReturnMetric(buffer_size = num_eval_episodes), #metric to compute av return\n",
    "        tf_metrics.AverageEpisodeLengthMetric(buffer_size = num_eval_episodes) #metric to compute av ep length\n",
    "    ]\n",
    "    \n",
    "    #Create global step\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "    \n",
    "    with tf.compat.v2.summary.record_if(\n",
    "        lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "        eval_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "        eval_py_env = suite_gym.load(env_name)\n",
    "    \n",
    "    \n",
    "        #Define Actor Network\n",
    "        actorNN = actor_network.ActorNetwork(\n",
    "                  tf_env.time_step_spec().observation,\n",
    "                  tf_env.action_spec(),\n",
    "                  fc_layer_params=(400, 300),\n",
    "        )\n",
    "    \n",
    "        #Define Critic Network\n",
    "        NN_input_specs = (tf_env.time_step_spec().observation,\n",
    "                          tf_env.action_spec()\n",
    "        )\n",
    "    \n",
    "        criticNN = critic_network.CriticNetwork(\n",
    "                   NN_input_specs,\n",
    "                   observation_fc_layer_params = (400,),\n",
    "                   action_fc_layer_params = None,\n",
    "                   joint_fc_layer_params = (300,),\n",
    "        )\n",
    "        \n",
    "        #Define & initialize DDPG Agent\n",
    "        agent = ddpg_agent.DdpgAgent(\n",
    "                tf_env.time_step_spec(),\n",
    "                tf_env.action_spec(),\n",
    "                actor_network = actorNN,\n",
    "                critic_network = criticNN,\n",
    "                actor_optimizer = tf.compat.v1.train.AdamOptimizer(\n",
    "                                  learning_rate = actor_learning_rate),\n",
    "                critic_optimizer = tf.compat.v1.train.AdamOptimizer(\n",
    "                                   learning_rate = critic_learning_rate),\n",
    "                ou_stddev = ou_stddev,\n",
    "                ou_damping = ou_damping,\n",
    "                target_update_tau = target_update_tau,\n",
    "                target_update_period = target_update_period,\n",
    "                dqda_clipping = None,\n",
    "                td_errors_loss_fn = tf.compat.v1.losses.mean_squared_error,\n",
    "                gamma = gamma,\n",
    "                reward_scale_factor = 1.0,\n",
    "                gradient_clipping = None,\n",
    "                debug_summaries = False,\n",
    "                summarize_grads_and_vars = False,\n",
    "                train_step_counter = global_step\n",
    "        )\n",
    "        agent.initialize()\n",
    "        \n",
    "        #Determine which train metrics to display with summary writer\n",
    "        train_metrics = [\n",
    "            tf_metrics.NumberOfEpisodes(),\n",
    "            tf_metrics.EnvironmentSteps(),\n",
    "            tf_metrics.AverageReturnMetric(),\n",
    "            tf_metrics.AverageEpisodeLengthMetric(),\n",
    "        ]\n",
    "        \n",
    "        #Set policies for evaluation and initial collection\n",
    "        eval_policy = agent.policy #actor policy\n",
    "        collect_policy = agent.collect_policy #actor policy with OUNoise\n",
    "        \n",
    "        #Set up replay buffer\n",
    "        replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "                        agent.collect_data_spec,\n",
    "                        batch_size = tf_env.batch_size,\n",
    "                        max_length = replay_buffer_capacity\n",
    "        )\n",
    "        \n",
    "        #Define driver for initial replay buffer filling\n",
    "        initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "                                     tf_env,\n",
    "                                     collect_policy,\n",
    "                                     observers = [replay_buffer.add_batch],\n",
    "                                     num_steps = initial_collect_steps\n",
    "        )\n",
    "\n",
    "        #Define collect driver for collect steps per iteration\n",
    "        collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "                             tf_env,\n",
    "                             collect_policy,\n",
    "                             observers = [replay_buffer.add_batch] + train_metrics,\n",
    "                             num_steps = collect_steps_per_iteration\n",
    "        )\n",
    "        \n",
    "        if use_tf_functions:\n",
    "            initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "            collect_driver.run = common.function(collect_driver.run)\n",
    "            agent.train = common.function(agent.train)\n",
    "            \n",
    "        # Collect initial replay data\n",
    "        logging.info(\n",
    "            'Initializing replay buffer by collecting experience for %d steps with '\n",
    "            'a random policy.', initial_collect_steps)\n",
    "        initial_collect_driver.run()\n",
    "        \n",
    "        #Computes Evaluation Metrics\n",
    "        results = metric_utils.eager_compute(\n",
    "                  eval_metrics,\n",
    "                  eval_tf_env,\n",
    "                  eval_policy,\n",
    "                  num_episodes = num_eval_episodes,\n",
    "                  train_step = global_step,\n",
    "                  summary_writer = eval_summary_writer,\n",
    "                  summary_prefix = 'Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "            eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "        \n",
    "        time_step = None\n",
    "        policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0 \n",
    "\n",
    "        # Dataset generates trajectories with shape [Bx2x...]\n",
    "        dataset = replay_buffer.as_dataset(\n",
    "                  num_parallel_calls = 3,\n",
    "                  sample_batch_size = 64,\n",
    "                  num_steps = 2).prefetch(3)\n",
    "        iterator = iter(dataset)\n",
    "\n",
    "        def train_step():\n",
    "            global counter\n",
    "            global experience\n",
    "            if counter % 2 == 0:\n",
    "                experience, _ = next(iterator) #Get experience from dataset\n",
    "                counter += 1\n",
    "            return agent.train(experience) #Train agent on that experience\n",
    "        \n",
    "        if use_tf_functions:\n",
    "            train_step = common.function(train_step)\n",
    "            \n",
    "        \n",
    "        #Where the magic happens\n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time() #Get start time\n",
    "            #Collect some data for replay buffer (also observed by train metrics)\n",
    "            time_step, policy_state = collect_driver.run(\n",
    "                                      time_step = time_step,\n",
    "                                      policy_state = policy_state,\n",
    "            )\n",
    "            #Train on collected experience n times\n",
    "            for _ in range(train_steps_per_iteration):\n",
    "                train_loss = train_step()\n",
    "            time_acc += time.time() - start_time\n",
    "\n",
    "            if global_step.numpy() % log_interval == 0:\n",
    "                logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                             train_loss.loss\n",
    "                )\n",
    "                steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "                logging.info('%.3f steps/sec', steps_per_sec)\n",
    "                tf.compat.v2.summary.scalar(\n",
    "                    name = 'global_steps_per_sec', data = steps_per_sec, \n",
    "                    step = global_step\n",
    "                )\n",
    "                timed_at_step = global_step.numpy()\n",
    "                time_acc = 0\n",
    "\n",
    "            for train_metric in train_metrics:\n",
    "                train_metric.tf_summaries(train_step = global_step, \n",
    "                                          step_metrics = train_metrics[:2])\n",
    "                \n",
    "            if global_step.numpy() % eval_interval == 0:\n",
    "                results = metric_utils.eager_compute(\n",
    "                          eval_metrics,\n",
    "                          eval_tf_env,\n",
    "                          eval_policy,\n",
    "                          num_episodes = num_eval_episodes,\n",
    "                          train_step = global_step,\n",
    "                          summary_writer = eval_summary_writer,\n",
    "                          summary_prefix = 'Metrics',\n",
    "                )\n",
    "                if eval_metrics_callback is not None:\n",
    "                    eval_metrics_callback(results, global_step.numpy())\n",
    "                metric_utils.log_metrics(eval_metrics)\n",
    "                if results['AverageReturn'].numpy() >= 300.0:\n",
    "                    vid_py_env = gym.make(env_name)\n",
    "                    vid_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
    "                    vid_py_env = wrappers.Monitor(vid_py_env, video_dir + \"/\" + str(global_step.numpy()) + \"/\")\n",
    "                    driver = dynamic_step_driver.DynamicStepDriver(vid_env, eval_policy, num_steps = 1)\n",
    "                    time_step = vid_env.reset()\n",
    "                    policy_state = eval_policy.get_initial_state(vid_env.batch_size)\n",
    "                    state = vid_py_env.reset()\n",
    "                    score = 0\n",
    "                    while True:\n",
    "                        vid_py_env.render()\n",
    "                        action_step = eval_policy.action(time_step, policy_state)\n",
    "                        action_tens = action_step[0].numpy()\n",
    "                        next_state, reward, done, _ = vid_py_env.step(action_tens[0])\n",
    "                        #time_step = vid_env.step(action_step)  \n",
    "                        score += reward\n",
    "                        time_step, policy_state = driver.run(time_step, policy_state)\n",
    "                        if time_step.is_last():\n",
    "                            break\n",
    "                        if done:\n",
    "                            break\n",
    "                    if score >= 250:\n",
    "                        print(str(global_step.numpy()))\n",
    "                    vid_py_env.close()\n",
    "                    vid_env.close()\n",
    "    return train_loss\n",
    "\n",
    "DDPG_Bipedal(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
