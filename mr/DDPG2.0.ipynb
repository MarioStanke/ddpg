{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This Program implements DDPG using TF Agents Library. \n",
    "It also creates training evaluation and evaluation data.\n",
    "That data can be viewed using tensorboard (i.e. tensorboard --logdir=eval).\n",
    "Update: It now also creates videos of agent performance every 50000/100000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove superfluous imports\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "\n",
    "import gin\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from tf_agents.agents.ddpg import actor_network\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from gym.envs import box2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\m-ohm\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\m-ohm\\Miniconda3\\envs\\tf2rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#TODO: Adjust params so that agent learns properly.\n",
    "#Likely culprits: Target update params, critic network\n",
    "#13-12: corrected env_load_fn, adjusted params according to CNN3, smaller critic net\n",
    "#IDEAS: Increase collect steps per iteration?\n",
    "#test 15-12: Train steps per iteration, collect steps\n",
    "#Note: code does not work with lunarlander or cartpole (float input error at line 83 (critic_net))\n",
    "\n",
    "def train_eval(\n",
    "    root_dir,\n",
    "    env_name='BipedalWalker-v2',\n",
    "    eval_env_name=None,\n",
    "    env_load_fn=suite_gym.load,\n",
    "    num_iterations=2000000,\n",
    "    actor_fc_layers=(400, 300),\n",
    "    critic_obs_fc_layers=(256,),\n",
    "    critic_action_fc_layers=(256,),\n",
    "    critic_joint_fc_layers=(128,),\n",
    "    # Params for collect\n",
    "    initial_collect_steps=1000,\n",
    "    collect_steps_per_iteration=10, #1\n",
    "    num_parallel_environments=1,\n",
    "    replay_buffer_capacity=100000,\n",
    "    ou_stddev=0.2, #0.2\n",
    "    ou_damping=0.15, #0.15\n",
    "    # Params for target update\n",
    "    target_update_tau=0.001, #0.05\n",
    "    target_update_period=1, #5\n",
    "    # Params for train\n",
    "    train_steps_per_iteration=20, #1/10\n",
    "    batch_size=64,\n",
    "    actor_learning_rate=1e-4,\n",
    "    critic_learning_rate=1e-3,\n",
    "    dqda_clipping=None,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.huber_loss, #tf.compat.v1.losses.huber_loss potential problem? MSE?\n",
    "    gamma=0.99, #0.995\n",
    "    reward_scale_factor=1.0,\n",
    "    gradient_clipping=None,\n",
    "    use_tf_functions=True,\n",
    "    # Params for eval\n",
    "    num_eval_episodes=10,\n",
    "    eval_interval=10000,\n",
    "    # Params for checkpoints, summaries, and logging\n",
    "    log_interval=1000,\n",
    "    summary_interval=1000,\n",
    "    video_interval=100000,\n",
    "    summaries_flush_secs=10,\n",
    "    debug_summaries=False,\n",
    "    summarize_grads_and_vars=False,\n",
    "    eval_metrics_callback=None\n",
    "    ):\n",
    "\n",
    "  \"\"\"A simple train and eval for DDPG.\"\"\"\n",
    "  root_dir = os.path.expanduser(root_dir)\n",
    "  train_dir = os.path.join(root_dir, 'train2.0')\n",
    "  eval_dir = os.path.join(root_dir, 'eval2.0')\n",
    "  video_dir = os.path.join(root_dir, 'vid2.0')\n",
    "\n",
    "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  train_summary_writer.set_as_default()\n",
    "\n",
    "  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
    "      eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "  eval_metrics = [\n",
    "      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "  ]\n",
    "\n",
    "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "  with tf.compat.v2.summary.record_if(\n",
    "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    if num_parallel_environments > 1:\n",
    "      tf_env = tf_py_environment.TFPyEnvironment(\n",
    "          parallel_py_environment.ParallelPyEnvironment(\n",
    "              [lambda: env_load_fn(env_name)] * num_parallel_environments))\n",
    "    else:\n",
    "      tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n",
    "    eval_env_name = eval_env_name or env_name\n",
    "    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(eval_env_name))\n",
    "\n",
    "    actor_net = actor_network.ActorNetwork(\n",
    "        tf_env.time_step_spec().observation,\n",
    "        tf_env.action_spec(),\n",
    "        fc_layer_params=actor_fc_layers,\n",
    "    )\n",
    "\n",
    "    critic_net_input_specs = (tf_env.time_step_spec().observation,\n",
    "                              tf_env.action_spec())\n",
    "\n",
    "    critic_net = critic_network.CriticNetwork(\n",
    "        critic_net_input_specs,\n",
    "        observation_fc_layer_params=critic_obs_fc_layers,\n",
    "        action_fc_layer_params=critic_action_fc_layers,\n",
    "        joint_fc_layer_params=critic_joint_fc_layers,\n",
    "    )\n",
    "\n",
    "    tf_agent = ddpg_agent.DdpgAgent(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        ou_stddev=ou_stddev,\n",
    "        ou_damping=ou_damping,\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=td_errors_loss_fn,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "    tf_agent.initialize()\n",
    "\n",
    "    train_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        tf_metrics.EnvironmentSteps(),\n",
    "        tf_metrics.AverageReturnMetric(),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(),\n",
    "    ]\n",
    "\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        tf_agent.collect_data_spec,\n",
    "        batch_size=tf_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch],\n",
    "        num_steps=initial_collect_steps)\n",
    "\n",
    "    collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_steps=collect_steps_per_iteration)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "      collect_driver.run = common.function(collect_driver.run)\n",
    "      tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Collect initial replay data.\n",
    "    logging.info(\n",
    "        'Initializing replay buffer by collecting experience for %d steps with '\n",
    "        'a random policy.', initial_collect_steps)\n",
    "    initial_collect_driver.run()\n",
    "\n",
    "    results = metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n",
    "    if eval_metrics_callback is not None:\n",
    "      eval_metrics_callback(results, global_step.numpy())\n",
    "    metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    time_step = None\n",
    "    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "    timed_at_step = global_step.numpy()\n",
    "    time_acc = 0\n",
    "\n",
    "    # Dataset generates trajectories with shape [Bx2x...]\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3,\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=2).prefetch(3)\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    def train_step():\n",
    "      experience, _ = next(iterator)\n",
    "      return tf_agent.train(experience)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      train_step = common.function(train_step)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "      start_time = time.time()\n",
    "      time_step, policy_state = collect_driver.run(\n",
    "          time_step=time_step,\n",
    "          policy_state=policy_state,\n",
    "      )\n",
    "      for _ in range(train_steps_per_iteration):\n",
    "        train_loss = train_step()\n",
    "      time_acc += time.time() - start_time\n",
    "\n",
    "      if global_step.numpy() % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step.numpy(),\n",
    "                     train_loss.loss)\n",
    "        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "        timed_at_step = global_step.numpy()\n",
    "        time_acc = 0\n",
    "\n",
    "      for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=train_metrics[:2])\n",
    "        \n",
    "      # Make video frames   \n",
    "      if global_step.numpy() % video_interval == 0:\n",
    "        tempenv = gym.make('BipedalWalker-v2')\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n",
    "        tempenv = wrappers.Monitor(tempenv, video_dir + \"/\" + str(global_step.numpy()) +\"/\")\n",
    "        driver = dynamic_step_driver.DynamicStepDriver(\n",
    "        tf_env, tf_agent.policy, num_steps=1)\n",
    "        time_step = tf_env.reset()\n",
    "        state = tempenv.reset()\n",
    "        score = 0\n",
    "        while True:\n",
    "                time_step, policy_state = driver.run(time_step)\n",
    "                action_step = tf_agent.policy.action(time_step, policy_state)\n",
    "                fnally=action_step[0].numpy()\n",
    "                next_state, reward, done, _ = tempenv.step(fnally[0])\n",
    "                time_step = tf_env.step(action_step)\n",
    "                tempenv.render()                \n",
    "                score += reward\n",
    "                state = next_state\n",
    "                if time_step.is_last():\n",
    "                    break\n",
    "                if done:\n",
    "                    break\n",
    "      \n",
    "        tempenv.close()\n",
    "        tf_env.close()\n",
    "\n",
    "      if global_step.numpy() % eval_interval == 0:\n",
    "        results = metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "        if eval_metrics_callback is not None:\n",
    "          eval_metrics_callback(results, global_step.numpy())\n",
    "        metric_utils.log_metrics(eval_metrics)\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "root_dir = '~/Bachelorarbeit' #adjust root dir to your own\n",
    "train_eval(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python <myenv>",
   "language": "python",
   "name": "tf2rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
